Please find below the R code history from your *Wallace* v1.0.6.2
session.

You can reproduce your session results by running this R Markdown file
in RStudio.

Each code block is called a “chunk”, and you can run them either
one-by-one or all at once by choosing an option in the “Run” menu at the
top-right corner of the “Source” pane in RStudio.

For more detailed information see
<a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>).

### Package installation

Wallace uses the following R packages that must be installed and loaded
before starting.

```{r}
library(spocc)
library(spThin)
library(dismo)
library(rgeos)
library(ENMeval)
library(dplyr)
```

Wallace also includes several functions developed to help integrate
different packages and some additional functionality. For this reason,
it is necessary to load the file `functions.R`, The function
`system.file()` finds this script, and `source()` loads it.

```{r}
source(system.file('shiny/funcs', 'functions.R', package = 'wallace'))
```

Record of analysis for \*\*.
----------------------------

User CSV path with occurrence data. If the CSV file is not in the
current workspace, change to the correct file path
(e.g. “/Users/darwin/Documents/occs.csv”).

```{r}
userOccs.csv <- A.alle
# remove rows with duplicate coordinates
occs.dups <- duplicated(userOccs.csv[c('longitude', 'latitude')])
occs <- userOccs.csv[!occs.dups,]
# remove NAs
occs <- occs[complete.cases(occs$longitude, occs$latitude), ]
# give all records a unique ID
occs$occID <- row.names(occs)
```

### Process Occurrence Data

Spatial thinning selected. Thin distance selected is 10 km.

```{r}
output <- spThin::thin(occs, 'latitude', 'longitude', 'name', thin.par = 10, reps = 100, locs.thinned.list.return = TRUE, write.files = FALSE, verbose = FALSE)
```

Since spThin did 100 iterations, there are 100 different variations of
how it thinned your occurrence localities. As there is a stochastic
element in the algorithm, some iterations may include more localities
than the others, and we need to make sure we maximize the number of
localities we proceed with.

```{r}
# find the iteration that returns the max number of occurrences
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))
# if there's more than one max, pick the first one
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]]  
# subset occs to match only thinned occs
occs <- occs[as.numeric(rownames(maxThin)),]  
```

### Obtain Environmental Data

```{r}
# NOTE: provide the path to the folder that contains the rasters
d.envs <- 'C:/Users/savan/Documents/github/Wallace/bio-oracle'
# create paths to the raster files
userRas.paths <- file.path(d.envs, c('Present.Surface.Chlorophyll.Mean.tif', 'Present.Surface.Current.Velocity.Mean.tif.BOv2_1.tif', 'Present.Surface.Dissolved.oxygen.Mean.tif', 'Present.Surface.Ice.cover.Mean.tif', 'Present.Surface.Ice.thickness.Mean.tif', 'Present.Surface.Iron.Mean.tif', 'Present.Surface.Nitrate.Mean.tif', 'Present.Surface.Phosphate.Mean.tif', 'Present.Surface.Phytoplankton.Mean.tif', 'Present.Surface.Primary.productivity.Mean.tif', 'Present.Surface.Salinity.Mean.tif', 'Present.Surface.Silicate.Mean.tif', 'Present.Surface.Temperature.Max.tif', 'Present.Surface.Temperature.Mean.tif', 'Present.Surface.Temperature.Min.tif'))
# make a RasterStack out of the raster files
envs <- raster::stack(userRas.paths)
```

### Process Environmental Data

Background selection technique chosen as Minimum Convex Polygon.

```{r}
occs.xy <- occs[c('longitude', 'latitude')]
sp::coordinates(occs.xy) <- ~ longitude + latitude
bgExt <- mcp(occs.xy)
```

Buffer size of the study extent polygon defined as 0.5 degrees.

```{r}
bgExt <- rgeos::gBuffer(bgExt, width = 0.5)
```

Mask environmental variables by Minimum Convex Polygon, and take a
random sample of background values from the study extent. As the sample
is random, your results may be different than those in the session. If
there seems to be too much variability in these background samples, try
increasing the number from 10,000 to something higher (e.g. 50,000 or
100,000). The better your background sample, the less variability you’ll
have between runs.

```{r}
# crop the environmental rasters by the background extent shape
envsBgCrop <- raster::crop(envs, bgExt)
# mask the background extent shape from the cropped raster
envsBgMsk <- raster::mask(envsBgCrop, bgExt)
# sample random background points
bg.xy <- dismo::randomPoints(envsBgMsk, 10000)
# convert matrix output to data frame
bg.xy <- as.data.frame(bg.xy)  
```

### Partition Occurrence Data

Occurrence data is now partitioned for cross-validation, a method that
iteratively builds a model on all but one group and evaluates that model
on the left-out group.

For example, if the data is partitioned into 3 groups A, B, and C, a
model is first built with groups A and B and is evaluated on C. This is
repeated by building a model with B and C and evaluating on A, and so on
until all combinations are done.

Cross-validation operates under the assumption that the groups are
independent of each other, which may or may not be a safe assumption for
your dataset. Spatial partitioning is one way to ensure more
independence between groups.

You selected to partition your occurrence data by the method.

```{r}
occs.xy <- occs[c('longitude', 'latitude')]
group.data <- ENMeval::get.randomkfold(occ=occs.xy, bg.coords=bg.xy, kfolds=2)
```

```{r}
# pull out the occurrence and background partition group numbers from the list
occs.grp <- group.data[[1]]
bg.grp <- group.data[[2]]
```

### Build and Evaluate Niche Model

You selected the maxent model.

```{r}
# define the vector of regularization multipliers to test
rms <- seq(1, 2, 1)
# iterate model building over all chosen parameter settings
e <- ENMeval::ENMevaluate(occs.xy, envsBgMsk, bg.coords = bg.xy, RMvalues = rms, fc = 'L', 
                          method = 'user', occs.grp, bg.grp, clamp = TRUE, algorithm = "maxnet")

# unpack the results data frame, the list of models, and the RasterStack of raw predictions
evalTbl <- e@results
evalMods <- e@models
names(evalMods) <- e@results$settings
evalPreds <- e@predictions
```

```{r}
# view response curves for environmental variables with non-zero coefficients
plot(evalMods[["L_1"]], vars = c('Present.Surface.Phosphate.Mean', 'Present.Surface.Phytoplankton.Mean', 'Present.Surface.Silicate.Mean'), type = "cloglog")
```

```{r}
# view ENMeval results
ENMeval::eval.plot(evalTbl, value = "delta.AICc")
```

```{r}
# Select your model from the models list
mod <- evalMods[["L_1"]]
```

```{r}
# generate raw prediction
pred <- evalPreds[["L_1"]]
```

```{r}
# plot the model prediction
plot(pred)
```

### Project Niche Model

You selected to project your model. First define a polygon with the
coordinates you chose, then crop and mask your predictor rasters.
Finally, predict suitability values for these new raster cells based on
the model you selected.

```{r}
projCoords <- data.frame(x = c(-78.7972, -65.6108, 39.5284, 27.2211, -78.7972), y = c(32.7836, 71.1651, 72.4806, 31.1432, 32.7836))
projPoly <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(projCoords)), ID=1)))
```

### Project Niche Model to New Extent

Now use crop and mask the predictor variables by projPoly, and use the
maxnet.predictRaster() function to predict the values for the new extent
based on the model selected.

```{r}
predsProj <- raster::crop(envs, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
proj <- ENMeval::maxnet.predictRaster(mod, predsProj, type = 'exponential', clamp = TRUE)
```

```{r}
# plot the model prediction
plot(proj)
```
